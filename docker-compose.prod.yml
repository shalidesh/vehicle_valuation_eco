version: '3.8'

#-------Common configuration--------------

x-airflow-common:
  &airflow-common
  build: ./airflow
  environment:
    &airflow-common-env
    # need parallel DAG execution -> LocalExecutor
    AIRFLOW__CORE__EXECUTOR: LocalExecutor 
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres/${AIRFLOW_DB_NAME:-airflow_db}
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    AIRFLOW__EMAIL__EMAIL_BACKEND: airflow.utils.email.send_email_smtp
    AIRFLOW__SMTP__SMTP_HOST: smtp.gmail.com
    AIRFLOW__SMTP__START_TLS: True
    AIRFLOW__SMTP__SMTP_SSL: False
    AIRFLOW__SMTP__SMTP_USER: sdeshan970bis@gmail.com
    AIRFLOW__SMTP__SMTP_PASSWORD: pkzimsktajdvvzip
    AIRFLOW__SMTP__SMTP_PORT: 587
    AIRFLOW__SMTP__SMTP_MAIL_FROM: sdeshan970bis@gmail.com
    # need parallel DAG execution uncomment below
    AIRFLOW__CORE__PARALLELISM: 4
    AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: 2
    AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY: 8
    # App database connection for DAG tasks (scraping data goes here)
    APP_DB_HOST: postgres
    APP_DB_PORT: 5432
    APP_DB_NAME: ${APP_DB_NAME:-cdb_valuation_db}
    APP_DB_USER: ${POSTGRES_USER:-airflow}
    APP_DB_PASSWORD: ${POSTGRES_PASSWORD:-airflow}
    FLARESOLVERR_URL: http://flaresolverr:8191/v1

  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    postgres:
      condition: service_healthy

services:
 #---------airflow pipeline-------------

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
    volumes:
      - ./airflow:/sources
    networks:
      - app_network

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - app_network
    mem_limit: 1.5g
    cpus: 1.0

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - app_network
    mem_limit: 2g
    cpus: 1.5

  # airflow-triggerer:
  #   <<: *airflow-common
  #   command: triggerer
  #   healthcheck:
  #     test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 30s
  #   restart: always
  #   depends_on:
  #     <<: *airflow-common-depends-on
  #     airflow-init:
  #       condition: service_completed_successfully
  #   networks:
  #     - app_network

  # airflow-cli:
  #   <<: *airflow-common
  #   profiles:
  #     - debug
  #   environment:
  #     <<: *airflow-common-env
  #     CONNECTION_CHECK_MAX_COUNT: "0"
  #   # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
  #   command:
  #     - bash
  #     - -c
  #     - airflow
  #   networks:
  #   - app_network

# -------------------- Database Layer --------------------
  postgres:
    image: postgres:13
    container_name: cdb_postgres
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: ${AIRFLOW_DB_NAME:-airflow_db}
      APP_DB_NAME: ${APP_DB_NAME:-cdb_valuation_db}
      POSTGRES_SHARED_BUFFERS: 256MB
      POSTGRES_WORK_MEM: 16MB
      POSTGRES_MAINTENANCE_WORK_MEM: 64MB
    mem_limit: 1.5g
    volumes:
      - postgres_data:/var/lib/postgresql/data:rw
      - ./docker/postgres-init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-airflow} -d ${AIRFLOW_DB_NAME:-airflow_db}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - app_network

  # pgadmin:
  #   image: dpage/pgadmin4
  #   container_name: pgadmin
  #   restart: unless-stopped
  #   environment:
  #     PGADMIN_DEFAULT_EMAIL: "admin@cdb.lk"   # Set admin email
  #     PGADMIN_DEFAULT_PASSWORD: "admin123"    # Set pgAdmin password
  #   ports:
  #     - "5050:80"
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #   networks:
  #     - app_network

#-------------------Scraped Data Service--------------------

  flaresolverr: 
    image: 21hsmw/flaresolverr:nodriver 
    container_name: flaresolverr 
    ports: 
      - "8191:8191" 
    restart: unless-stopped
    networks:
      - app_network

# -------------------- Migration Service --------------------
  db_migrations:
    build:
      context: ./web_portal/backend
      dockerfile: Dockerfile.migrations
    container_name: cdb_migrations
    environment:
      - DATABASE_URL=postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres:5432/${APP_DB_NAME:-cdb_valuation_db}
      - SECRET_KEY=${BACKEND_SECRET_KEY:-6619d0f5a1b0980135bb15788ee4101ae886a5508144d2b2a5342232c39b7495892f6a0539dee3d0445593d1ce9ebbdec7e2687fcf6c5a3a5f651af6c0a4919e}
    depends_on:
      postgres:
        condition: service_healthy
    restart: on-failure
    networks:
      - app_network

# -------------------- Backend Services --------------------
  backend:
    build:
      context: ./web_portal/backend
      dockerfile: Dockerfile.prod
    container_name: cdb_backend
    restart: always
    environment:
      - DATABASE_URL=postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres:5432/${APP_DB_NAME:-cdb_valuation_db}
      - SECRET_KEY=${BACKEND_SECRET_KEY:-6619d0f5a1b0980135bb15788ee4101ae886a5508144d2b2a5342232c39b7495892f6a0539dee3d0445593d1ce9ebbdec7e2687fcf6c5a3a5f651af6c0a4919e}
      - ACCESS_TOKEN_EXPIRE_MINUTES=${ACCESS_TOKEN_EXPIRE_MINUTES:-30}
      - REFRESH_TOKEN_EXPIRE_DAYS=${REFRESH_TOKEN_EXPIRE_DAYS:-7}
      - DB_USER=${POSTGRES_USER:-airflow}
      - DB_PASSWORD=${POSTGRES_PASSWORD:-airflow}
      - DB_NAME=${APP_DB_NAME:-cdb_valuation_db}
      - DB_HOST=postgres
      - DB_PORT=5432
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost,http://localhost:3000}
    volumes:
      - backend_logs:/app/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    depends_on:
      postgres:
        condition: service_healthy
      db_migrations:
        condition: service_completed_successfully
    networks:
      - app_network

# -------------------- AI Valuation Engine --------------------
  valuation_engine:
    build:
      context: ./api_uat
      dockerfile: Dockerfile
    container_name: cdb_valuation_engine
    restart: always
    environment:
      - DATABASE_URL=postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres:5432/${APP_DB_NAME:-cdb_valuation_db}
      - SECRET_KEY=${VALUATION_SECRET_KEY:-thisismynewversionscreatversion}
      - AUTH_USERNAME=${VALUATION_USER:-cdb}
      - AUTH_PASSWORD=${VALUATION_PASSWORD:-cdb123456}
      - FIXED_ACCESS_TOKEN=${VALUATION_FIXED_TOKEN:-cdb_fixed_token_2024_secure_key_valuation_engine}
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost,http://localhost:3000}
      - DEBUG=false
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8443/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - app_network
    # For production scaling: uncomment below
    # deploy:
    #   replicas: 2

# -------------------- Frontend --------------------
  frontend:
    build:
      context: ./web_portal/frontend
      dockerfile: Dockerfile.prod
      args:
        - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost/backend}
    container_name: cdb_frontend
    restart: always
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost/backend}
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - app_network

# -------------------- Reverse Proxy --------------------
  nginx:
    image: nginx:alpine
    container_name: cdb_nginx
    restart: always
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      # Uncomment for SSL certificates
      # - /path/to/ssl/certs:/etc/nginx/ssl:ro
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      frontend:
        condition: service_healthy
      backend:
        condition: service_healthy
      valuation_engine:
        condition: service_healthy
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3

# -------------------- Volumes --------------------
volumes:
  postgres_data:
    driver: local
  backend_logs:
    driver: local
  valuation_logs:
    driver: local

# -------------------- Networks --------------------
networks:
  app_network:
    driver: bridge
